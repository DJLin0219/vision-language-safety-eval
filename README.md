# SafeVLM: Fine-Tuning and Evaluation Framework for Vision-Language Models

This repository provides a **sample implementation** inspired by my prior internship work on **vision-language model (VLM) benchmarking and fine-tuning** for safety-critical perception tasks (e.g., traffic light and scene understanding).  
All data and code are **synthetic** and intended solely for **research demonstration** purposes.

---

## ğŸ” Overview

This project presents a reproducible framework for **fine-tuning**, **evaluating**, and **benchmarking** modern Vision-Language Models (VLMs) on perception-aligned tasks.  
The design focuses on *robustness under visual uncertainty* and *cross-modal consistency*, resembling the core ideas used in large-scale safety perception evaluation pipelines.

### Key Features
- ğŸ§  **VLM Integration Layer** â€” unified interface for models such as LLaVA-NeXT, PaliGemma, Fuyu, InternVideo2.
- ğŸ¯ **Fine-tuning Pipeline** â€” modular PyTorch pipeline for supervised or instruction-tuned adaptation using multimodal data.
- ğŸ“ˆ **Evaluation Metrics** â€” supports visual-textual accuracy, consistency, and reasoning alignment metrics.
- ğŸ§© **Prompt and Response Benchmarking** â€” evaluates LLM reasoning coherence under degraded perception (e.g., low-light or occluded scenes).
- âš¡ **Fully open and synthetic** â€” no private datasets or internal assets are included.

---

## ğŸ§° Repository Structure
